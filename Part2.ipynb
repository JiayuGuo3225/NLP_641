{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## Part 2: Word Embeddings and Neural Network\n",
    "\n",
    "In this notebook you will learn a powerful method to represent word in the numerical way and apply it to a simply 2-layer network for classification.\n",
    "\n",
    "**Outline**:\n",
    "\n",
    "- Neural Network\n",
    "- Word Embeddings\n",
    "\n",
    "\n",
    "**Pipeline**\n",
    "\n",
    "<img src=\"resources/pipeline.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "For this section, we will introduce some basis about neural network and define a basic NN using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to build a model in Tensorflow:\n",
    "\n",
    "1. Define a new Model class from `nn.Module` base class. Override `__init__` and `forward`.\n",
    "2. Define a `nn.Sequential` and add layers one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import utils and set plt settings\n",
    "import nlp_proj_utils2 as utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Make and load Moon Dataset\n",
    "train_x, test_x, train_y, test_y = utils.load_moon()\n",
    "\n",
    "plt.scatter(\n",
    "    train_x[:,0],     # first feature as x\n",
    "    train_x[:,1],     # second feature as y\n",
    "    c=train_y.T[0],   # label as color\n",
    "    cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are \n",
    "print('type of train and test', type(train_x))\n",
    "print('shape of X', train_x.shape)\n",
    "print('shape of Y', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, Neural Network is the more general form of LR, which can be considered as a **1-Layer NN** (input layer doesn't count). \n",
    "\n",
    "<img src=\"resources/1-layer-nn.png\">\n",
    "\n",
    "<br>\n",
    "<center>A 1-layer neural network: Logistic Regression</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = Sequential()\n",
    "lr_model.add(Dense(1, input_dim=2, activation='sigmoid')) # output layer\n",
    "\n",
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='sgd',\n",
    "    metrics=['accuracy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_history = lr_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    epochs=500, # Intend to set a large number here for demonstration\n",
    "    validation_data=(test_x, test_y), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(lr_history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_decision_boundary(lr_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "LR works not so well when features are not linearly separable. It depends heavily on features, so feature engineering is essential if you are using LR.\n",
    "\n",
    "<img src=\"resources/2-layer-nn.png\">\n",
    "\n",
    "<br>\n",
    "<center>A 2-layer neural network: 1 hidden layer + 1 output layer</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn_model(input_dim, layers, output_dim):\n",
    "    # Input layer\n",
    "    X = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    H = X\n",
    "    for layer in layers:\n",
    "        H = Dense(layer, activation='relu')(H)\n",
    "    \n",
    "    # Output layer\n",
    "    activation_func = 'softmax' if output_dim > 1 else 'sigmoid'\n",
    "    \n",
    "    Y = Dense(output_dim, activation=activation_func)(H)\n",
    "    return Model(inputs=X, outputs=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = build_nn_model(\n",
    "    input_dim=2,\n",
    "    layers=[8],\n",
    "    output_dim=1\n",
    ")\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_history = nn_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    epochs=500, \n",
    "    validation_data=(test_x, test_y), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(nn_history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_decision_boundary(nn_model, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "<img src=\"resources/word-vector.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Classifier\n",
    "\n",
    "<img src=\"resources/emoji.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = utils.load_emoji()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load word embeddings\n",
    "# This util function returns two dict: word_to_index and word_to_vec\n",
    "# At this moment, we only need the second part\n",
    "_, word_to_vec_map = utils.load_glove_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 samples\n",
    "for i in range(5):\n",
    "    print(train_x[i], utils.label_to_emoji(train_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert output to one hot vector\n",
    "train_y_oh = utils.convert_to_one_hot(train_y, 5)\n",
    "test_y_oh = utils.convert_to_one_hot(test_y, 5)\n",
    "\n",
    "print(train_y[0], \"is converted into one hot\", train_y_oh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = utils.sentence_to_avg(\"I like it\", word_to_vec_map)\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(\n",
    "    [utils.sentence_to_avg(x, word_to_vec_map) for x in train_x])\n",
    "\n",
    "test_x = np.array(\n",
    "    [utils.sentence_to_avg(x, word_to_vec_map) for x in test_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_model = build_nn_model(\n",
    "    input_dim=50, \n",
    "    layers=[50], \n",
    "    output_dim=5)\n",
    "\n",
    "emoji_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_history = emoji_model.fit(\n",
    "    train_x, \n",
    "    train_y_oh, \n",
    "    epochs=500, \n",
    "    shuffle=True, \n",
    "    validation_data=(test_x, test_y_oh), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.plot_history(emoji_history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_emoji(text):\n",
    "    embedding = np.array([utils.sentence_to_avg(text, word_to_vec_map)]) # get embedding\n",
    "    pred = emoji_model.predict([embedding]) # predict, return the probability of each class\n",
    "    label = np.argmax(pred) # choose the one with largest probability as label\n",
    "    return utils.label_to_emoji(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    \"i love you\", \n",
    "    \"it's horrible\", \n",
    "    \"funny lol\", \n",
    "    \"lets play with a ball\", \n",
    "    \"food is ready\", \n",
    "    \"i don't like it\"]\n",
    "\n",
    "for test in tests:\n",
    "    print(test,pred_emoji(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
