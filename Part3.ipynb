{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis P3\n",
    "\n",
    "In this notebook, we will build a deep LSTM network and insert a fixed pre-trained embedding layer in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/pipeline.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import \n",
    "import numpy as np\n",
    "import nlp_proj_utils3 as utils\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = utils.load_emoji()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load pretrained word embeddings**\n",
    "\n",
    "2 dictionaries are loaded:\n",
    "\n",
    "- `word_to_index`: map a word to its index in the vocabulary\n",
    "    - Example:  `'word' -> 1234`\n",
    "\n",
    "- `word_to_vec_map`: map a word to its embedding\n",
    "    - Example: `'word' -> [0.1, 0.2, ..., 0.45]`\n",
    "\n",
    "When adding a custom embedding layer in Keras, we can only load the pretrained embedding as a big matrix instead of a dictionary. An index will help us locate the entry for a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings & One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, word_to_vec_map = utils.load_glove_vecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.sentences_to_indices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert word to the index in vocabulary\n",
    "utils.sentences_to_indices(\n",
    "    np.array([\"i like it\", \"i hate it\"]),  # array of test sentences\n",
    "    word_to_index, \n",
    "    max_len = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(x.split()) for x in train_x])\n",
    "print('max number of words in a sentence:', maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training/testing features into index list\n",
    "train_x = utils.sentences_to_indices(train_x, word_to_index, maxlen)\n",
    "test_x = utils.sentences_to_indices(test_x, word_to_index, maxlen)\n",
    "\n",
    "# Convert training/testing labels into one hot array\n",
    "train_y = utils.convert_to_one_hot(train_y, C = 5)\n",
    "test_y = utils.convert_to_one_hot(test_y, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to make sure the shape looks good\n",
    "assert train_x.shape == (132, maxlen)\n",
    "assert train_y.shape == (132, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer\n",
    "\n",
    "We need to build a embedding matrix where each row represent a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_index, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Build and return a Keras Embedding Layer given word_to_vec mapping and word_to_index mapping\n",
    "    \n",
    "    Args:\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "\n",
    "    Return:\n",
    "        Keras.layers.Embedding: Embedding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keras requires vocab length start from index 1\n",
    "    vocab_len = len(word_to_index) + 1  \n",
    "    emb_dim = list(word_to_vec_map.values())[0].shape[0]\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    return Embedding(\n",
    "        input_dim=vocab_len, \n",
    "        output_dim=emb_dim, \n",
    "        trainable=False,  # Indicating this is a pre-trained embedding \n",
    "        weights=[emb_matrix]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on how to define a pre-trained embedding layer in Keras, please refer to [this post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/deep_lstm.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> A 2-layer LSTM sequence classifier. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emoji_model(input_dim, word_to_index, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Build and return the Keras model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: The dim of input layer\n",
    "        word_to_vec_map (dict[str->np.ndarray]): map from a word to a vector with shape (N,) where N is the length of a word vector (50 in our case)\n",
    "        word_to_index (dict[str->int]): map from a word to its index in vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        Keras.models.Model: 2-layer LSTM model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    sentence_indices = Input(shape=(input_dim,), dtype='int32')\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_index, word_to_vec_map)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # 2-layer LSTM\n",
    "    X = LSTM(128, return_sequences=True, recurrent_dropout=0.5)(embeddings)  # N->N RNN\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    X = LSTM(128, recurrent_dropout=0.5)(X)  # N -> 1 RNN\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    X = Dense(5, activation='softmax')(X)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_model = build_emoji_model(\n",
    "    maxlen, \n",
    "    word_to_index, \n",
    "    word_to_vec_map)\n",
    "\n",
    "emoji_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = emoji_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    epochs = 100,  \n",
    "    # has to be a tuple, due to a tf bug: https://github.com/tensorflow/tensorflow/issues/39370\n",
    "    validation_data=(test_x, test_y)  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_history(history, ['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_model.evaluate(train_x, train_y)\n",
    "emoji_model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sava and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two parts need to be saved inorder to use the model in prod:\n",
    "\n",
    "1. Neural Network Structure\n",
    "2. Trained Weights (Matrix)\n",
    "\n",
    "We will save them separately. This makes it easy to manage multiple versions of weights and you can always choose which version to go for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_root = 'resources/emoji_model'\n",
    "os.makedirs(model_root, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model structure as json\n",
    "with open(os.path.join(model_root, \"network.json\"), \"w\") as fp:\n",
    "    fp.write(emoji_model.to_json())\n",
    "\n",
    "# Save model weights\n",
    "emoji_model.save_weights(os.path.join(model_root, \"weights.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load a pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_path, weights_path = utils.download_best_emoji_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Load model structure\n",
    "with open(network_path, \"r\") as fp:\n",
    "    emoji_model_best = model_from_json(fp.read())\n",
    "\n",
    "# Load model weights\n",
    "emoji_model_best.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_model_best.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    x = utils.sentences_to_indices(\n",
    "        np.array([text]), \n",
    "        word_to_index, \n",
    "        maxlen)\n",
    "    \n",
    "    probs = emoji_model_best.predict(x)\n",
    "    pred = np.argmax(probs)\n",
    "    \n",
    "    print(text, utils.label_to_emoji(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('i am not feeling happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
